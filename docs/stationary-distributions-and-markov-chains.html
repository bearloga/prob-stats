<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="6 Stationary Distributions and Markov Chains | Probability and Statistics" />
<meta property="og:type" content="book" />





<meta name="author" content="Bob Carpenter" />

<meta name="date" content="2019-01-01" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="6 Stationary Distributions and Markov Chains | Probability and Statistics">

<title>6 Stationary Distributions and Markov Chains | Probability and Statistics</title>

<link href="libs/tufte-css/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css/tufte.css" rel="stylesheet" />





</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="what-is-probability.html#what-is-probability">What is Probability?</a></li>
<li><a href="random-variables-and-event-probabilities.html#random-variables-and-event-probabilities"><span class="toc-section-number">1</span> Random Variables and Event Probabilities</a></li>
<li><a href="multiple-random-variables-and-probability-functions.html#multiple-random-variables-and-probability-functions"><span class="toc-section-number">2</span> Multiple Random Variables and Probability Functions</a></li>
<li><a href="expectations-and-variance.html#expectations-and-variance"><span class="toc-section-number">3</span> Expectations and Variance</a></li>
<li><a href="joint-marginal-and-conditional-probabilities.html#joint-marginal-and-conditional-probabilities"><span class="toc-section-number">4</span> Joint, Marginal, and Conditional Probabilities</a></li>
<li><a href="finite-state-markov-chains.html#finite-state-markov-chains"><span class="toc-section-number">5</span> Finite-State Markov Chains</a></li>
<li><a href="stationary-distributions-and-markov-chains.html#stationary-distributions-and-markov-chains"><span class="toc-section-number">6</span> Stationary Distributions and Markov Chains</a></li>
<li><a href="infinite-discrete-markov-chains.html#infinite-discrete-markov-chains"><span class="toc-section-number">7</span> Infinite Discrete Markov Chains</a></li>
<li><a href="continuous-random-variables.html#continuous-random-variables"><span class="toc-section-number">8</span> Continuous Random Variables</a></li>
<li><a href="continuous-distributions-and-densities.html#continuous-distributions-and-densities"><span class="toc-section-number">9</span> Continuous Distributions and Densities</a></li>
<li><a href="statistical-inference-and-inverse-problems.html#statistical-inference-and-inverse-problems"><span class="toc-section-number">10</span> Statistical Inference and Inverse Problems</a></li>
<li><a href="rejection-sampling.html#rejection-sampling"><span class="toc-section-number">11</span> Rejection Sampling</a></li>
<li><a href="posterior-predictive-inference.html#posterior-predictive-inference"><span class="toc-section-number">12</span> Posterior Predictive Inference</a></li>
<li><a href="pseudorandom-number-generators.html#pseudorandom-number-generators"><span class="toc-section-number">13</span> Pseudorandom Number Generators</a></li>
<li><a href="floating-point-arithmetic.html#floating-point-arithmetic"><span class="toc-section-number">14</span> Floating Point Arithmetic</a></li>
<li><a href="normal-distribution.html#normal-distribution"><span class="toc-section-number">15</span> Normal Distribution</a></li>
<li><a href="calibration-and-sharpness.html#calibration-and-sharpness"><span class="toc-section-number">16</span> Calibration and Sharpness</a></li>
<li><a href="regression-to-the-mean.html#regression-to-the-mean"><span class="toc-section-number">17</span> Regression to the Mean</a></li>
<li><a href="markov-chain-monte-carlo-methods.html#markov-chain-monte-carlo-methods"><span class="toc-section-number">18</span> Markov Chain Monte Carlo Methods</a></li>
<li><a href="random-walk-metropolis.html#random-walk-metropolis"><span class="toc-section-number">19</span> Random Walk Metropolis</a></li>
<li><a href="monitoring-approximate-convergence.html#monitoring-approximate-convergence"><span class="toc-section-number">20</span> Monitoring Approximate Convergence</a></li>
<li><a href="exponential-and-poisson-distributions.html#exponential-and-poisson-distributions"><span class="toc-section-number">21</span> Exponential and Poisson Distributions</a></li>
<li><a href="conjugate-priors.html#conjugate-priors"><span class="toc-section-number">22</span> Conjugate Priors</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="stationary-distributions-and-markov-chains" class="section level1">
<h1><span class="header-section-number">6</span> Stationary Distributions and Markov Chains</h1>
<div id="stationary-markov-chains" class="section level2">
<h2><span class="header-section-number">6.1</span> Stationary Markov chains</h2>
<p>A random process <span class="math inline">\(Y = Y_1, Y_2, \ldots\)</span> is said to be <em>stationary</em> if
the marginal probability of a sequence of elements does not depend on
where it starts in the chain. In symbols, a discrete-time random
process <span class="math inline">\(Y\)</span> is stationary if for any <span class="math inline">\(t \geq 1\)</span> and any sequence
sequence <span class="math inline">\(u_1, \ldots, u_N \in \mathbb{R}^N\)</span> of size <span class="math inline">\(N\)</span>, we have</p>
<p><span class="math display">\[
p_{Y_1, \ldots, Y_N}(u_1, \ldots, u_N)
= p_{Y_t, \ldots, Y_{t + N}}(u_1, \ldots, u_N)
\]</span></p>
<p>None of the chains we will construct for practical applications will
be stationary in this sense, because we would need to know the
appropriate initial distribution <span class="math inline">\(p_{Y_1}(y_1)\)</span>. For example,
consider the fishes example in which we know the trandition
probabilities, but not the stationary distribution. If we run long
enough, the proportion of pike stabilizes</p>
</div>
<div id="stationary-distributions" class="section level2">
<h2><span class="header-section-number">6.2</span> Stationary distributions</h2>
<p>Although we will not, in practice, have Markov chains that are
stationary from <span class="math inline">\(t = 1\)</span>, we will use Markov chains that have
stationary distributions in the limit as <span class="math inline">\(t \rightarrow \infty\)</span>. For
a Markov chain to be stationary, there must be some <span class="math inline">\(q\)</span> such that</p>
<p><span class="math display">\[
p_{Y_t}(u) = q(u)
\]</span></p>
<p>for all <span class="math inline">\(t\)</span>, starting from <span class="math inline">\(t = 0\)</span>. Instead, we will have an
equilibrium distribution <span class="math inline">\(q\)</span> that the chain approaches in the limit as
<span class="math inline">\(t\)</span> grows. In symbols,</p>
<p><span class="math display">\[
\lim_{t \rightarrow \infty}
\
p_{Y_t}(u) = q(u).
\]</span></p>
<p>Very confusingly, this equilibrium distribution <span class="math inline">\(q\)</span> is also called a
<em>stationary distribution</em> in the Markov chain literature, so we will
stick to that nomenclature. We never truly arrive at <span class="math inline">\(q(u)\)</span> for a
finite <span class="math inline">\(t\)</span> because of the bias introduced by the initial distribution
<span class="math inline">\(p_{Y_1}(u) \neq q(u)\)</span>. Nevertheless, as with our earlier
simulation-based estimates, we can get arbitrarily close after
suitably many iterations.<label for="tufte-sn-96" class="margin-toggle sidenote-number">96</label><input type="checkbox" id="tufte-sn-96" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">96</span> The last section of this chapter
illustrates rates of convergence to the stationary distribution, but
the general discussion is in the later chapter on continuous-state
Markov chains.</span></p>
<p>Reconsider the example of a process <span class="math inline">\(Y = Y_1, Y_2, \ldots\)</span> of fishes,
where 1 represents a pike and 0 a perch. We assumed the Markov process
<span class="math inline">\(Y\)</span> was governed by</p>
<p><span class="math display">\[
\begin{array}{rcl}
\mbox{Pr}[Y_{t + 1} = 1 \mid Y_t = 1] &amp; = &amp; 0.20
\\[4pt]
\mbox{Pr}[Y_{t + 1} = 1 \mid Y_t = 0] &amp; = &amp; 0.05
\end{array}
\]</span></p>
<p>Rewriting as a probability mass function,</p>
<p><span class="math display">\[
p_{Y_{t + 1} \mid Y_t}(j \mid i) = \theta_{i, j},
\]</span></p>
<p>where <span class="math inline">\(\theta_{i, j}\)</span> is the probabiltiy of a transtion to state <span class="math inline">\(j\)</span>
given that the process is in state <span class="math inline">\(i\)</span>. For the pike and perch
example, <span class="math inline">\(\theta\)</span> is fully defined by</p>
<p><span class="math display">\[
\begin{array}{rcl}
\theta_{1, 1} &amp; = &amp; 0.20
\\
\theta_{1, 2} &amp; = &amp; 0.80
\\ \hline
\theta_{2, 1} &amp; = &amp; 0.05
\\
\theta_{2, 2} &amp; = &amp; 0.95.
\end{array}
\]</span></p>
<p>These numbers are normally displayed in the form of a <em>transition
matrix</em>, which records the transitions out of each state as a row,
with the column indicating the target state,</p>
<p><span class="math display">\[
\theta =
\begin{bmatrix}
0.20 &amp; 0.80 \\
0.05 &amp; 0.95
\end{bmatrix}.
\]</span></p>
<p>The first row of this transition matrix is <span class="math inline">\((0.20, 0.80)\)</span> and the
second row is <span class="math inline">\((0.05, 0.95)\)</span>. Rows of transition matrices will always
have non-negative entries and sum to one, because they are the
parameters to categorical distributions.<label for="tufte-sn-97" class="margin-toggle sidenote-number">97</label><input type="checkbox" id="tufte-sn-97" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">97</span> Vectors with non-negative
values that sum to one are known as <em>unit simplexes</em> and matrices in
which every row is a unit simplex is said to be a <em>stochastic matrix.</em>
Transition matrices for finite-state Markov chains are always
stochastic matrices.</span></p>
<p>Now let’s take a really long run of the chain with <span class="math inline">\(T = 1\,000\,000\)</span>
fish to get a precise estimate of the long-run proportion of pike.</p>
<pre><code>   initial state = 0.000000; simulated proportion of pike = 0.059
   initial state = 1.000000; simulated proportion of pike = 0.059</code></pre>
<p>The initial state doesn’t seem to matter. That’s because the rate of
5.9% pike is the stationary distribution. More
formally, let <span class="math inline">\(\pi = (0.059, 1 - 0.059)\)</span> and note that<label for="tufte-sn-98" class="margin-toggle sidenote-number">98</label><input type="checkbox" id="tufte-sn-98" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">98</span> In matrix
notation, if <span class="math inline">\(\pi\)</span> is considered a row vector, then <span class="math display">\[\pi = \theta \, \pi.\]</span></span></p>
<p><span class="math display">\[
\pi_i = \sum_{j = 1}^2 \pi_j \times \theta_{j, i}.
\]</span></p>
<p>If <span class="math inline">\(\pi\)</span> satisfies this formula, then it is said to be the <em>stationary
distribution</em> for <span class="math inline">\(\theta.\)</span></p>
<p>If a Markov chain has a stationary distribution <span class="math inline">\(\pi\)</span> and the initial
distribution of <span class="math inline">\(Y_1\)</span> is also <span class="math inline">\(\pi\)</span>, then it is stationary.</p>
</div>
<div id="reducible-chains" class="section level2">
<h2><span class="header-section-number">6.3</span> Reducible chains</h2>
<p>The Markov chains we will use for sampling from target distributions
will be well behaved by construction. There are, however, things that
can go wrong with Markov chains that prevent them from having
stationary distributions. The first of these is reducibility. A
chain is reducible if it can get stuck in a state from which other
states are not guaranteed to be revisited with probability one.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-61"></span>
<p class="caption marginnote shownote">
Figure 6.1: State diagram for a reducible finite Markov chain. The chain will eventually get stuck in state 3 and never exit to visit states 1 or 2 again.
</p>
<img src="_main_files/figure-html/unnamed-chunk-61-1.pdf" alt="State diagram for a reducible finite Markov chain.  The chain will eventually get stuck in state 3 and never exit to visit states 1 or 2 again." width="45%"  />
</div>
<p>If we start the chain in state 1, it will eventually transition to
state 3 and get stuck there.<label for="tufte-sn-99" class="margin-toggle sidenote-number">99</label><input type="checkbox" id="tufte-sn-99" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">99</span> State 3 is what is known as a <em>sink
state.</em></span> It’s not necessary to get stuck in a single state. The same
problem arises if state 3 has transitions out, as long as they can’t
eventually get back to state 1.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-62"></span>
<p class="caption marginnote shownote">
Figure 6.2: State diagram for another reducible finite Markov chain. The chain will eventually get stuck in state 3 and 4 and never exit to visit states 1 or 2 again.
</p>
<img src="_main_files/figure-html/unnamed-chunk-62-1.pdf" alt="State diagram for another reducible finite Markov chain.  The chain will eventually get stuck in state 3 and 4 and never exit to visit states 1 or 2 again." width="65%"  />
</div>
<p>In this example, the chain will eventually fall into a state where it
can only visit states 3 and 4.</p>
</div>
<div id="periodic-chains" class="section level2">
<h2><span class="header-section-number">6.4</span> Periodic chains</h2>
<p>A Markov chain can be constructed to cycle through states in a regular
(probabilistic) pattern. For example, consider the following Markov
chain transitions.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-63"></span>
<p class="caption marginnote shownote">
Figure 6.3: State diagram for finite Markov chain generating periodic state sequences <span class="math inline">\(\ldots, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, \ldots\)</span>.
</p>
<img src="_main_files/figure-html/unnamed-chunk-63-1.pdf" alt="State diagram for finite Markov chain generating periodic state sequences $\ldots, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, \ldots$." width="35%"  />
</div>
<p>Regular cycles like this defeat the existence of a stationary
distribution. If <span class="math inline">\(Y_1 = 2\)</span>, the entire chain is deterministically
defined to be</p>
<p><span class="math display">\[
Y = 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, \ldots.
\]</span></p>
<p>Clearly <span class="math inline">\(p_{Y_t} \neq p_{Y_{t+1}}\)</span>, as each concentrates all of its
probability mass on a different value.</p>
<p>On the other hand, this chain is what is known as wide-state
stationary in that using long-running frequency estimates are stable.
the expected value is <span class="math inline">\(\frac{1 + 2 + 3}{3} = 2\)</span> and the standard
deviatiation is <span class="math inline">\(\sqrt{\frac{1^2 + 0^2 + 1^2}{3}} \approx 0.47\)</span>. More
formally, the wide-state expectation is calculated as</p>
<p><span class="math display">\[
\lim_{T \rightarrow \infty}
\
\frac{1}{T}
\sum_{t=1}^T Y_t
\rightarrow 2.
\]</span></p>
<p>The definition of periodicity is more subtle than just deterministic
chains. For example, the following transition graph is also periodic.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-64"></span>
<p class="caption marginnote shownote">
Figure 6.4: State diagram for finite Markov chain generating periodic state sequences alternatiing between state 1 and either state 2 or state 3.
</p>
<img src="_main_files/figure-html/unnamed-chunk-64-1.pdf" alt="State diagram for finite Markov chain generating periodic state sequences alternatiing between state 1 and either state 2 or state 3." width="45%"  />
</div>
<p>Rather than a deterministic cycle, it cycles between the state 1 and
the pair of states 2 and 3. A simulation might look like</p>
<p><span class="math display">\[
y^{(1)} = 1, 2, 1, 2, 1, 2, 1, 3, 1, 3, 1, 2, 1, 3, 1, 2, \ldots
\]</span></p>
<p>Every other value is a 1, no matter whether the chain starts in state
1, 2, or 3. Such behavior means there’s no stationary distribution.
But there is a wide-sense stable probability estimate for the states,
namely 50% of the time spent in state 1, and 25% of the time spent in
each of states 2 and 3.</p>
</div>
<div id="convergence-of-finite-state-chains" class="section level2">
<h2><span class="header-section-number">6.5</span> Convergence of finite-state chains</h2>
<p>In applied statistics, we proceed by simulation, running chains long
enough that they provide stable long-term frequency estimates. These
stable long-term frequency estimates are of the stationary
distribution <span class="math inline">\(\mbox{categorical}(\pi)\)</span>. All of the Markov chains we
construct to sample from target distributions of interest (e.g.,
Bayesian posterior predictive distributions) will be well-behaved in
that these long-term frequency estimates will be stable, in
theory.<label for="tufte-sn-100" class="margin-toggle sidenote-number">100</label><input type="checkbox" id="tufte-sn-100" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">100</span> In practice, we will have to be very careful with diagnostics
to avoid poor behavior due to floating-point arithmetic combined
with approximate numerical algorithms.</span></p>
<p>In practice, none of the Markov chains we employ in calculations will
be stationary for the simple technical reason that we don’t know the
stationary distribution ahead of time and thus cannot draw <span class="math inline">\(Y_1\)</span> from
it.<label for="tufte-sn-101" class="margin-toggle sidenote-number">101</label><input type="checkbox" id="tufte-sn-101" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">101</span> In the finite case, we actually can calculate it either through
simulation or as the eigenvector of the transition matrix with
eigenvalue one (which is guaranteed to exist). An eigenvector of a
matrix is a row vector <span class="math inline">\(\pi\)</span> such that <span class="math display">\[c \times \pi = \theta \,
\pi,\]</span> where <span class="math inline">\(c\)</span> is the eigenvalue. This is why Google’s PageRank
algorithm is known to computational statisticians as the “billion
dollar eigenvector.” One way to calculate the relevant eigenvector of
a stochastic matrix is by raising it to a power, starting from any
non-degenerate initial simplex vector <span class="math inline">\(\lambda\)</span>, <span class="math display">\[\lim_{n \rightarrow
\infty} \lambda \, \theta^n = \pi.\]</span> Each <span class="math display">\[\theta^n =
\underbrace{\theta \times \theta \times \cdots \times
\theta}_{\textstyle n \ \mbox{times}}\]</span> is a transition matrix
corresponding to taking <span class="math inline">\(n\)</span> steps in the original transition matrix
<span class="math inline">\(\theta\)</span>.</span> What we need to know is conditions under which a Markov
chain will “forget” its initial state after many steps and converge to
the stationary distribution.</p>
<p>All of the Markov chains we will employ for applied statistics
applications will be well behaved in the sense that when run long
enough, the distribution of each element in the chain will approach
the stationary distribution. Roughly, when <span class="math inline">\(t\)</span> is large enough, the
marginal distribution <span class="math inline">\(p_{Y_t}\)</span> stabilizes to the stationary
distribution. The well-behavedness conditions required for this to
hold may be stated as follows</p>
<p><strong>Fundamental Convergence Theorem.</strong> If a Markov chain <span class="math inline">\(Y = Y_1, Y_2, \ldots\)</span> is (a) irreducible, (b) aperiodic, and (c) has a
stationary distribution <span class="math inline">\(\mbox{categorical}(\pi)\)</span>, then</p>
<p><span class="math display">\[
\lim_{t \rightarrow \infty}
\
P_{Y_t}(u) \rightarrow \mbox{categorical}(u \mid \pi).
\]</span></p>
<p>What this means in practice is that we can use a single simulation,</p>
<p><span class="math display">\[
y^{(1)}
\ = \
y^{(1)}_1, y^{(1)}_2, \ldots, y^{(1)}_T
\]</span></p>
<p>to estimate the parameters for the stationary distribution. More
specifically, if we define <span class="math inline">\(\pi\)</span> by</p>
<p><span class="math display">\[
\widehat{\pi}_i
=
\frac{1}{T} \sum_{t = 1}^T \mathrm{I}[y_t^{(1)} = i]
\]</span></p>
<p>then we can estimate the stationary distribution as
<span class="math inline">\(\mbox{categorical}(\widehat{\pi}).\)</span></p>
<p>As a coherence check, we often run a total of <span class="math inline">\(M\)</span> simulations of the
first <span class="math inline">\(T\)</span> values of the Markov chain <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[
\begin{array}{rcl}
y^{(1)} &amp; = &amp; y_1^{(1)}, y_2^{(1)}, \ldots, y_T^{(1)}
\\[4pt]
y^{(2)} &amp; = &amp; y_1^{(2)}, y_2^{(2)}, \ldots, y_T^{(2)}
\\[2pt]
\vdots
\\[2pt]
y^{(M)} &amp; = &amp; y_1^{(M)}, y_2^{(M)}, \ldots, y_T^{(M)}
\end{array}
\]</span></p>
<p>We should get the same estimate from using <span class="math inline">\(y^{(m)}\)</span> from a single
simulation <span class="math inline">\(m\)</span> as we get from using all of the simulated chains
<span class="math inline">\(y^{(1)}, \ldots, y^{(M)}\)</span>.<label for="tufte-sn-102" class="margin-toggle sidenote-number">102</label><input type="checkbox" id="tufte-sn-102" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">102</span> We’d expect lower error from using all of
the chains as we have a larger sample with which to estimate.</span></p>
</div>
<div id="how-fast-is-convergence" class="section level2">
<h2><span class="header-section-number">6.6</span> How fast is convergence?</h2>
<p>The fundamental theorem tells us that if a Markov chain <span class="math inline">\(Y = Y_1, Y_2, \ldots\)</span> is ergodic (aperiodic and irreducible) and has a stationary
distribution, then the distribution of <span class="math inline">\(Y_t\)</span> converges to the
stationary distribution in rhw limit. But it doesn’t tell us how fast.</p>
<p>As with everything else, we’ll go at this by simulation to establish
intuitions. In particular, we’ll consider three chains that have
<span class="math inline">\(\mbox{bernoulli}(0.5)\)</span> as their stationary distribution (a fair coin
toss).</p>
<p>First, we will consider a Markov chain producing independent Bernoulli
draws.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-65"></span>
<p class="caption marginnote shownote">
Figure 6.5: State diagram for finite Markov chain generating independent draws.
</p>
<img src="_main_files/figure-html/unnamed-chunk-65-1.pdf" alt="State diagram for finite Markov chain generating independent draws." width="25%"  />
</div>
<p>Whether it is currently in state 0 or state 1, there is a 50% chance
the next state is 0 and a 50% chance it is 1. Thus each element of the
process is generated independently and is identically distributed,</p>
<p><span class="math display">\[
Y_t \sim \mbox{bernoulli}(0.5).
\]</span></p>
<p>Therefore, the stationary distribution must also be <span class="math inline">\(\pi = (0.5, 0.5)\)</span>,
because</p>
<p><span class="math display">\[
\begin{array}{rcl}
\pi_1 &amp; = &amp; \pi_1 \times \theta_{1, 1} + \pi_2 \times \theta_{2, 1}
\\[4pt]
0.5 &amp; = &amp; 0.5 \times 0.5 + 0.5 \times 0.5
\end{array}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{array}{rcl}
\pi_2 &amp; = &amp; \pi_1 \times \theta_{1, 2} + \pi_2 \times \theta_{2, 2}
\\[4pt]
0.5 &amp; = &amp; 0.5 \times 0.5 + 0.5 \times 0.5.
\end{array}
\]</span></p>
<p>We can simulate 100 values and print the first 99 to see what the
chain looks like.</p>
<pre><code>   1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 
   1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 
   0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1</code></pre>
<p>An initial segment of a Markov chain <span class="math inline">\(Y = Y_1, Y_2, \ldots, Y_T\)</span> can
be visualized as a traceplot, a line plot of the value at each
iteration.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-68"></span>
<p class="caption marginnote shownote">
Figure 6.6: Traceplot of chain producing independent draws, simulated for 100 time steps. The horizontal axis is time (<span class="math inline">\(t\)</span>) and the vertical axis the value of e iteration number and the value is the value (<span class="math inline">\(Y_t\)</span>).
</p>
<img src="_main_files/figure-html/unnamed-chunk-68-1.png" alt="Traceplot of chain producing independent draws, simulated for 100 time steps.  The horizontal axis is time ($t$) and the vertical axis the value of e iteration number and the value is the value ($Y_t$)." width="40%"  />
</div>
<p>The flat segments are runs of the same value. This Markov chain
occassionally has runs of the same value, but otherwise mixes quite
well between the values.</p>
<p>So how fast do estimates of the stationary distribution based on an
initial segment <span class="math inline">\(Y_1, \ldots, Y_T\)</span> converge to <span class="math inline">\(\frac{1}{2}\)</span>? Because
each <span class="math inline">\(Y_t\)</span> is independent and identically distributed, the central
limit theorem tells us that the rate of convergence, as measured by
standard deviation of the distribution of estimates, goes down as
<span class="math inline">\(\frac{1}{\sqrt{T}}\)</span> with an initial segment <span class="math inline">\(Y_1, \ldots, Y_T\)</span> of the
chain goes down in error as <span class="math inline">\(\sqrt{T}\)</span></p>
<p>Now consider a Markov chain which is still symmetric in the states,
but with a tendency to stay in the same state.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-69"></span>
<p class="caption marginnote shownote">
Figure 6.7: State diagram for correlated draws.
</p>
<img src="_main_files/figure-html/unnamed-chunk-69-1.pdf" alt="State diagram for correlated draws." width="25%"  />
</div>
<p>It has the same stationary distribution of 0.5. Letting <span class="math inline">\(\theta = \begin{bmatrix}0.9 &amp; 0.1 \\ 0.1 &amp; 0.9 \end{bmatrix}\)</span> be the transition
matrix and <span class="math inline">\(\pi = (0.5, 0.5)\)</span> be the probabilities of the stationary
distribution we see that the general formula is satisfied by this
Markov chain,</p>
<p><span class="math display">\[
\begin{array}{rcl}
\pi_1 &amp; = &amp; \pi_1 \times \theta_{1, 1} + \pi_2 \times \theta_{2, 1}
\\[4pt]
0.5 &amp; = &amp; 0.5 \times 0.9 + 0.5 \times 0.1
\end{array}
\]</span></p>
<p>The same relation holds for <span class="math inline">\(\pi_2\)</span>,</p>
<p><span class="math display">\[
\begin{array}{rcl}
\pi_2 &amp; = &amp; \pi_1 \times \theta_{1, 2} + \pi_2 \times \theta_{2, 2}
\\[4pt]
0.5 &amp; = &amp; 0.5 \times 0.1 + 0.5 \times 0.9
\end{array}
\]</span></p>
<p>We can simulate from the chain
and print the first 99 values, and then print the traceplot.</p>
<pre><code>   1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 
   1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-71"></span>
<p class="caption marginnote shownote">
Figure 6.8: Traceplot for chain with correlated draws.
</p>
<img src="_main_files/figure-html/unnamed-chunk-71-1.png" alt="Traceplot for chain with correlated draws." width="40%"  />
</div>
<p>As expected, there are now long runs of the same value being produced.
This leads to much poorer mixing and a longer time for estimates based
on the draws to converge.</p>
<p>Finally, we consider the opposite case of a symmetric chain that
favors moving to a new state each time step.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-72"></span>
<p class="caption marginnote shownote">
Figure 6.9: State diagram for anticorrelated draws.
</p>
<img src="_main_files/figure-html/unnamed-chunk-72-1.pdf" alt="State diagram for anticorrelated draws." width="25%"  />
</div>
<p>Sampling, printing, and plotting the values produces</p>
<pre><code>   1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 
   0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 
   0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-74"></span>
<p class="caption marginnote shownote">
Figure 6.10: Traceplot of chain with anticorrelated draws.
</p>
<img src="_main_files/figure-html/unnamed-chunk-74-1.png" alt="Traceplot of chain with anticorrelated draws." width="40%"  />
</div>
<p>The draws form a dramatic sawtooth pattern as as they alternate
between zero and one.</p>
<p>Now let’s see how quickly estimates based on long-run averages from
the chain converge in in a side-by-side comparison.
A single chain is enough to illustrate the dramatic differences.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-76"></span>
<p class="caption marginnote shownote">
Figure 6.11: Estimate of the stationary probability <span class="math inline">\(\pi_1\)</span> of state 1 as a function of <span class="math inline">\(t\)</span> under three conditions, correlated, independent, and anticorrelated transitions. For each condition, 25 simulations of a chain of size <span class="math inline">\(T = 10\,000\)</span> are generated and overplotted.
</p>
<img src="_main_files/figure-html/unnamed-chunk-76-1.png" alt="Estimate of the stationary probability $\pi_1$ of state 1 as a function of $t$ under three conditions, correlated, independent, and anticorrelated transitions.  For each condition, 25 simulations of a chain of size $T = 10\,000$ are generated and overplotted." width="90%" height="150%"  />
</div>
</div>
<div id="reversibility" class="section level2">
<h2><span class="header-section-number">6.7</span> Reversibility</h2>
<p>These simple Markov chains wind up being reversible, in that the
probability of being in a state <span class="math inline">\(i\)</span> and then transitioning to state
<span class="math inline">\(j\)</span> is the same as that of being in state <span class="math inline">\(j\)</span> and transitioning to
state <span class="math inline">\(i\)</span>. In symbols, a discrete-valued Markov chain <span class="math inline">\(Y = Y_1, Y_2, \ldots\)</span> is <em>reversible</em> with respect to <span class="math inline">\(\pi\)</span> if</p>
<p><span class="math display">\[
\pi_i \times \theta_{i, j}
\ = \
\pi_j \times \theta_{j, i}.
\]</span></p>
<p>Reversibility is sufficient for establishing the existence of a
stationary distribution.<label for="tufte-sn-103" class="margin-toggle sidenote-number">103</label><input type="checkbox" id="tufte-sn-103" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">103</span> If a discrete Markov chain is reversible
with respect to <span class="math inline">\(\pi\)</span>, then <span class="math inline">\(\pi\)</span> is also the unique stationary
distribution of the Markov chain.</span> Markov chains can have stationary
distributions without being reversible.<label for="tufte-sn-104" class="margin-toggle sidenote-number">104</label><input type="checkbox" id="tufte-sn-104" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">104</span> The reducible chains with we
saw earlier are examples with stationary distributions that are not
reversible.</span> But all of the Markov chains we consider for practical
applications will turn out to be reversible.</p>

</div>
</div>
<p style="text-align: center;">
<a href="finite-state-markov-chains.html"><button class="btn btn-default">Previous</button></a>
<a href="infinite-discrete-markov-chains.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
